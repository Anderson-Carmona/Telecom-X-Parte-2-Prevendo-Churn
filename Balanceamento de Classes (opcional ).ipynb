{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPRYmnXp/7qpQr4a0g5VVLG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"a8YwH7s6AzoA"},"outputs":[],"source":["Caso queira aprofundar a análise, aplique técnicas de balanceamento como undersampling ou oversampling. Em situações de forte desbalanceamento, ferramentas como o SMOTE podem ser úteis para gerar exemplos sintéticos da classe minoritária.\n","\n","Essa é uma excelente estratégia. Em problemas de classificação onde uma classe é muito mais frequente que a outra (como detecção de fraudes, diagnósticos médicos ou falhas em máquinas), o modelo tende a ficar \"preguiçoso\" e prever apenas a classe majoritária para alcançar uma acurácia alta, ignorando o que realmente importa.\n","Aqui está um detalhamento de como aplicar essas técnicas de forma eficiente:\n","________________________________________\n","1. Oversampling e SMOTE (Aumentando a Minoria)\n","O Oversampling consiste em aumentar o número de instâncias da classe minoritária. A forma mais simples é duplicar registros existentes, mas isso pode levar ao overfitting.\n","É aqui que entra o SMOTE (Synthetic Minority Over-sampling Technique). Em vez de apenas replicar dados, ele cria novos dados sintéticos baseados na vizinhança dos dados reais.\n","•\tComo funciona: Ele seleciona um ponto da classe minoritária, encontra seus vizinhos mais próximos e cria um novo ponto em algum lugar na linha entre eles.\n","•\tVantagem: Introduz uma maior diversidade nos dados, ajudando o modelo a generalizar melhor a fronteira de decisão.\n","________________________________________\n","2. Undersampling (Reduzindo a Maioria)\n","O Undersampling foca em reduzir o número de instâncias da classe majoritária para igualar a proporção.\n","•\tQuando usar: Útil quando você tem um volume de dados massivo e a classe majoritária tem muitos exemplos redundantes.\n","•\tRisco: Você pode acabar descartando informações valiosas que ajudariam o modelo a entender a variação da classe principal.\n","•\tTécnica recomendada: Tomek Links, que remove exemplos da classe majoritária que estão muito próximos da fronteira com a classe minoritária, \"limpando\" o ruído.\n","________________________________________\n","3. Cuidados Essenciais ao Balancear\n","Para que sua análise seja robusta, evite estes erros comuns:\n","•\tBalancear ANTES do Split: Nunca aplique SMOTE ou Undersampling no dataset inteiro. Você deve fazer o train_test_split primeiro e balancear apenas os dados de treino. Caso contrário, haverá vazamento de dados (data leakage) e seus resultados de teste serão artificialmente altos.\n","•\tMétricas de Avaliação: Esqueça a Acurácia. Em dados desbalanceados, use:\n","o\tPrecision-Recall Curve / F1-Score: Para focar no acerto da classe minoritária.\n","o\tMatriz de Confusão: Para visualizar onde o modelo está errando.\n","o\tAUC-ROC: Para entender a capacidade de distinção entre as classes.\n","________________________________________\n","Exemplo Prático (Python)\n","Se estiver usando Python, a biblioteca imblearn é o padrão ouro:\n","Python\n","from imblearn.over_sampling import SMOTE\n","from sklearn.model_selection import train_test_split\n","\n","# 1. Split primeiro\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 2. Aplica SMOTE apenas no treino\n","sm = SMOTE(random_state=42)\n","X_res, y_res = sm.fit_resample(X_train, y_train)\n","\n","# 3. Treine seu modelo com X_res e y_res\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Y0koqpiNA0uj"},"execution_count":null,"outputs":[]}]}